% !TEX encoding = UTF-8 Unicode
% -*- coding: UTF-8; -*-
% vim: set fenc=utf-8

\chapter{Introdução}

Neste capítulo serão apresentadas a visão geral e a motivação dessa monografia, além da contribuição proposta.

\section{Visão geral e Contextualização}

% Simulações computacionais...análises...hipóteses científicas...
Simulações computacionais tornaram-se predominantes e omnipresentes no dia-a-dia de cientistas\footnote{Daqui em diante denominados \textit{usuários}.}, sendo necessárias na realização de análises que utilizam modelos computacionais complexos que lidam com grandes volumes de dados~\cite{silva2015analyzing}, permitindo a exploração de dados específicos de domínio para apoiar esses usuários na validação de hipóteses científicas ou comportamentos peculiares.

% Muitos programas...consumem e produzem muitos dados...
Essas simulações geralmente envolvem a execução de muitos programas no domínio da aplicação os quais intensivamente consomem e produzem muitos dados, a maior parte dos quais é armazenada em uma miscelânea de formatos de arquivo também no domínio da aplicação~\cite{silva2015analyzing}; por exemplo, \abbrev{FITS}{Flexible Image Transport System} FITS~\cite{greisen2002representations} em astronomia, ou \abbrev{HDF}{Hierarchical Data Format} HDF5~\cite{hdfgroup2014hdf5} e \abbrev{NetCDF}{Network Common Data Form} NetCDF~\cite{rew1990netcdf} em dinâmica de fluidos computacionais.
% Longo tempo para ser executada...mesmo em PAD...
Uma grande parte dessas simulações computacionais de larga escala leva um longo tempo para ser executada, mesmo em ambientes de \abbrev{PAD}{Processamento em Alto Desempenho} PAD (Processamento em Alto Desempenho)~\cite{silva2017raw}, onde o poder de computação é amplificado através da utilização de diversos núcleos computacionais.

% Rastrear grandezas...somente no fim...programas ad-hoc...SGBD...
Durante as mesmas, usuários comumente precisam rastrear grandezas de interesse, tais como resíduos, tempo de execução e estimativas de erro, baseadas em elementos de dados relacionados de vários arquivos, a fim de controlar a sua execução ao máximo possível~\cite{silva2016situ}.
Entretanto, esse rastreio costuma ser realizado somente ao fim da simulação, o que \textit{a priori} não é prático para o usuário, já que essas simulações levam um tempo considerável de execução mesmo em PAD, como mencionado anteriormente~\cite{silva2017raw}; além disso, mesmo considerando que os arquivos produzidos sejam tipicamente apoiados por diversas linguagens de programação e bibliotecas, programas \textit{ad-hoc} precisam comumente ser desenvolvidos a fim da produção análises em grande escala, as quais se tornam ainda mais custosas se realizadas com \abbrev{SGBD}{Sistema de Gerenciamento de Banco de Dados} SGBDs (Sistemas de Gerenciamento de Banco de Dados), pois eles aumentam a complexidade de execução em termos de estruturação de dados e carregamento na memória~\cite{silva2015analyzing}.

\section{Motivação}%
\label{sec:motivacao}

% SGWfCs...paralelização...proveniência...
Para amenizar o custo de tempo de execução, as simulações computacionais citadas anteriormente podem ser gerenciadas por um \abbrev{SGWfC}{Sistema de Gerência de \textit{Workflows} Científicos} SGWfC (Sistema de Gerência de \textit{Workflows} Científicos) paralelo, tal como o Kepler~\cite{ludascher2006scientific} e o Pegasus~\cite{deelman2005pegasus}, o que faz com que se beneficiem da proveniência e do paralelismo de dados entre os diferentes programas que compõem o seu \textit{workflow}\footnote{Em tradução livre, ``fluxo de trabalho''. No entanto, daqui em diante continuaremos a utilizar \textit{workflow}, por ser um termo mais preciso.}~\cite{bux2013parallelization}.
% Workflows...dataflows(dt, ds, phi)...DAG...
Tais \textit{workflows} científicos são composições de tarefas de processamento de dados sequenciais e concorrentes, cuja ordem de execução é determinada com base nas interdependências entre os dados~\cite{bux2013parallelization}. Eles podem ser modelados através de um fluxo de dados (\textit{dataflow}) que descreve todos os conjuntos de dados, atributos de dados, transformações de dados e dependências entre os mesmos~\cite{silva2017raw}. Esses fluxos de dados são usualmente especificados na forma de um \abbrev{DAG}{Directed Acyclic Graph --- Grafo Direcionado Acíclico} DAG (grafo direcionado acíclico), em que tarefas (transformações de dados) individuais são representadas como nós, e conjuntos de dados são modelados como arestas direcionadas.

% Trade-offs entre usabilidade e poder de controle...
Os SGWfCs citados anteriormente são usualmente difíceis de serem configurados e utilizados pelos usuários; alguns deles tais como o Taverna~\cite{hull2006taverna} possuem uma boa ênfase em usabilidade, porém não compensam na paralelização e utilização de recursos computacionais distribuídos; perdendo, assim, sua efetividade.
% Dificuldade em não utilizar SGWfCs...
Eles facilitam o \textit{design}, a manutenção, a execução, o monitoramento e o rastreio do processamento de dados científicos. Sem suporte a rastreio, o desafio é analisar a propagação de dados de conjuntos de arquivos ao longo da execução da simulação, o que é bastante propenso a erros se realizado manualmente~\cite{silva2015analyzing}, pois são necessárioas: \( (i) \) a nomenclatura de arquivos de forma apropriada; \( (ii) \) escrever e gerenciar programas \textit{ad-hoc} para gerenciar cada transformação de dados e arquivos; \( (iii) \) mapear o fluxo das transformações de dados.

No entanto, eis um dilema: existem três tipos básicos de consultas\footnote{Isso será mais aprofundado no~\autoref{chap:trabalhos-relacionados}.} que são rotineiramente utilizadas:

\begin{enumerate}
    \item conteúdo de arquivo específico de domínio;
    \item múltiplos arquivos relacionados através de transformações de dados / programas de simulação; e
    \item elementos de dados relacionados de múltiplos arquivos
\end{enumerate}

O dilema é que --- até onde se tem conhecimento durante a publicação deste documento --- todos os SGWfCs conhecidos realizam apenas consultas do tipo \( 1 \) ou \( 2 \), como o FastBit~\cite{wu2009fastbit} que realiza apenas consultas do tipo 1, e como o Chiron~\cite{ogasawara2013chiron} que é capaz de realizar ambos os tipos de consulta. Nenhum deles é capaz de realizar consultas do tipo 3.

\section{Contribuição}

Desse modo, como vimos na última seção, a capacidade de realizar todos os três tipos de consultas citados ainda é um problema em aberto, sendo especialmente importante em situações de larga escala, onde muitos dados são manipulados intensivamente.
Para preencher esse gargalo, foi proposta a arquitetura \abbrev{ARMFUL}{Analysis of Raw Data from Multiple Files} ARMFUL (Analysis of Raw Data from Multiple Files)\cite{silva2017raw,silva2016situ}: com ela, todos os três tipos de consultas podem ser realizados. Ela funciona estendendo um SGWfC que suporta os tipos de consulta \( 1 \) e \( 2 \) com uma base de dados de proveniência cujo objetivo é guardar referências aos dados e elementos do domínio da aplicação selecionados pelos usuários para que eles possam ser devidamente rastreados suportando, assim, consultas do tipo 3~\cite{silva2015analyzing}.

Dessa forma, os usuários podem submeter consultas para selecionar dados científicos de suas simulações científicas não apenas \textbf{após} mas também \textbf{durante} a execução da simulação; o objetivo é a possibilidade de captação os elementos de dados de interesse no momento em que eles são gerados~\cite{silva2015analyzing}. O benefício dessa abordagem é evitar que a a ação da extração (\textit{parsing}) dos dados tenha que ocorrer por completo; em vez disso, ela pode ser realizada dinamicamente (\textit{on-the-fly}), sem ter que fazer todo o carregamento dos dados em memória ou em um SGBD, o que evita a duplicação e a manutenção de dados no SGBD, e também reduz o custo computacional da consulta, que passa a ser realizada em menos tempo.

Mais especificamente, a contribuição desse trabalho é uma extensão da arquitetura ARMFUL apresentada em~\cite{silva2016situ,silva2017raw} com a introdução de um processador de consultas\footnote{Daqui em diante nomeado \textit{Query Processor} (QP).}.
O \abbrev{QP}{Query Processor --- Processador de Consultas} \textit{Query Processor} (QP) atua como um componente do ARMFUL que é responsável pela interação do usuário com os conjuntos de dados de suas simulações. A partir dele, o usuário é capaz de escrever uma consulta sem ter que aprender \abbrev{SQL}{Structured Query Language} SQL (Structured Query Language), pois ela é representada em alto nível, em um formato que é posteriormente traduzido (convertido) para SQL de forma transparente para o usuário. O benefício imediato disso é diminuir a curva de aprendizado para que uma consulta possa ser realizada, pois o usuário precisa se preocupar \textit{a priori} apenas com o domínio da aplicação da simulação computacional e com os dados e grandezas de interesse que ele deseja consultar.

Outro benefício é o desempenho atingido com as consultas: o \textit{Query Processor} faz o pré-processamento --- que só precisa ser realizado uma única vez, sempre que o QP é executado durante uma sessão --- em estruturas de dados eficientes de toda a proveniência gerada pelo \textit{ProvenanceGatherer}, de modo que consultas consecutivas beneficiam-se da proveniência já carregada na memória; agilizando, portanto, a velocidade da consulta.

\section{Estruturação do Texto}

O principal objetivo desse trabalho é apresentar o \textit{Query Processor}: como ele foi criado, como ele se integra à arquitetura ARMFUL previamente apresentada, e quais são os benefícios --- qualitativamente e quantitativamente --- que ele acrescenta à mesma.

Assim, ele está organizado da seguinte forma:
o \autoref{chap:referencial-teorico} apresenta o embasamento teórico, introduzindo e definindo os conceitos e a terminologia que será utilizada ao longo do trabalho.
No \autoref{chap:trabalhos-relacionados} listamos algumas publicações relacionadas e recentes, algumas dentre as quais serviram como motivação e base para essa.
O \autoref{chap:arquitetura-armful} aborda a arquitetura ARMFUL, da qual a aplicação desenvolvida nesse trabalho é um componente.
No \autoref{chap:rastros-de-proveniencia} desenvolvemos a abordagem utilizada para analisar rastros de proveniência dos arquivos e dados de simulações científicas.
No \autoref{chap:experimentos} são abordados os resultados obtidos através do \textit{Query Processor} e os experimentos realizados com o mesmo.
Finalmente, o \autoref{chap:conclusao} conclui o trabalho com perspectivas de trabalhos futuros.
