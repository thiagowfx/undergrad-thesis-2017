% Encoding: UTF-8

@Article{mcphillips2015yesworkflow,
  author   = {McPhillips, Timothy and Song, Tianhong and Kolisnik, Tyler and Aulenbach, Steve and Belhajjame, Khalid and Bocinsky, Kyle and Cao, Yang and Chirigati, Fernando and Dey, Saumen and Freire, Juliana and others},
  title    = {YesWorkflow: a user-oriented, language-independent tool for recovering workflow information from scripts},
  journal  = {arXiv preprint arXiv:1502.02403},
  year     = {2015},
  doi      = {10.2218/ijdc.v10i1.370},
  url      = {https://arxiv.org/abs/1502.02403},
  abstract = {Scientific workflow management systems offer features for composing complex computational pipelines from modular building blocks, for executing the resulting automated workflows, and for recording the provenance of data products resulting from workflow runs. Despite the advantages such features provide, many automated workflows continue to be implemented and executed outside of scientific workflow systems due to the convenience and familiarity of scripting languages (such as Perl, Python, R, and MATLAB), and to the high productivity many scientists experience when using these languages. YesWorkflow is a set of software tools that aim to provide such users of scripting languages with many of the benefits of scientific workflow systems. YesWorkflow requires neither the use of a workflow engine nor the overhead of adapting code to run effectively in such a system. Instead, YesWorkflow enables scientists to annotate existing scripts with special comments that reveal the computational modules and dataflows otherwise implicit in these scripts. YesWorkflow tools extract and analyze these comments, represent the scripts in terms of entities based on the typical scientific workflow model, and provide graphical renderings of this workflow-like view of the scripts. Future versions of YesWorkflow also will allow the prospective provenance of the data products of these scripts to be queried in ways similar to those available to users of scientific workflow systems.},
  file     = {online:https\://arxiv.org/pdf/1502.02403.pdf:PDF},
}

@InProceedings{murta2014noworkflow,
  author       = {Murta, Leonardo and Braganholo, Vanessa and Chirigati, Fernando and Koop, David and Freire, Juliana},
  title        = {noWorkflow: capturing and analyzing provenance of scripts},
  booktitle    = {International Provenance and Annotation Workshop},
  year         = {2014},
  organization = {Springer},
  pages        = {71--83},
  doi          = {10.1007/978-3-319-16462-5_6},
  url          = {https://link.springer.com/chapter/10.1007/978-3-319-16462-5_6},
  abstract     = {We propose noWorkflow, a tool that transparently captures
provenance of scripts and enables reproducibility. Unlike existing approaches,
noWorkflow is non-intrusive and does not require users to
change the way they work – users need not wrap their experiments in
scientific workflow systems, install version control systems, or instrument
their scripts. The tool leverages Software Engineering techniques, such as
abstract syntax tree analysis, reflection, and profiling, to collect different
types of provenance, including detailed information about the underlying
libraries. We describe how noWorkflow captures multiple kinds of
provenance and the different classes of analyses it supports: graph-based
visualization; differencing over provenance trails; and inference queries.},
  file         = {online:https\://vgc.poly.edu/~fchirigati/papers/murta-ipaw2014.pdf:PDF},
}

@InBook{Pimentel2016,
  author    = {Pimentel, Jo{\~a}o Felipe and Dey, Saumen and McPhillips, Timothy and Belhajjame, Khalid and Koop, David and Murta, Leonardo and Braganholo, Vanessa and Lud{\"a}scher, Bertram},
  title     = {Yin {\&} Yang: Demonstrating Complementary Provenance from noWorkflow {\&} YesWorkflow},
  booktitle = {Provenance and Annotation of Data and Processes: 6th International Provenance and Annotation Workshop, IPAW 2016, McLean, VA, USA, June 7-8, 2016, Proceedings},
  year      = {2016},
  editor    = {Mattoso, Marta and Glavic, Boris},
  publisher = {Springer International Publishing},
  isbn      = {978-3-319-40593-3},
  pages     = {161--165},
  doi       = {10.1007/978-3-319-40593-3_13},
  url       = {http://dx.doi.org/10.1007/978-3-319-40593-3_13},
  abstract  = {The noWorkflow and YesWorkflow toolkits both enable researchers to
capture, store, query, and visualize the provenance of results produced by scripts
that process scientific data. noWorkflow captures prospective provenance representing
the program structure of Python scripts, and retrospective provenance
representing key events observed during script execution. YesWorkflow captures
prospective provenance declared through annotations in the comments of scripts,
and supports key retrospective provenance queries by observing what files were
used or produced by the script. We demonstrate how combining complementary
information gathered by noWorkflow and YesWorkflow enables provenance
queries and data lineage visualizations neither tool can provide on its own.},
  address   = {Cham},
  file      = {online:https\://www.researchgate.net/profile/Vanessa_Braganholo/publication/301650411_Yin_Yang_Demonstrating_Complementary_Provenance_from_noWorkflow_YesWorkflow/links/571fe59d08aeaced788acd1f.pdf:PDF},
}

@InProceedings{silva2016situ,
  author    = {Silva, V{\'\i}tor and Camata, Jos{\'e} and De Oliveira, Daniel and Coutinho, Alvaro and Valduriez, Patrick and Mattoso, Marta},
  title     = {In Situ Data Steering on Sedimentation Simulation with Provenance Data},
  booktitle = {SC: High Performance Computing, Networking, Storage and Analysis},
  year      = {2016},
  url       = {https://hal-lirmm.ccsd.cnrs.fr/lirmm-01400532/},
  abstract  = {Parallel adaptive mesh refinement and coarsening
(AMR) are optimal strategies for tackling large-scale simulations.
libMesh is an open-source finite-element library that supports
parallel AMR and is used in multiphysics applications. In
complex simulation runs, users have to track quantities of
interest (residuals, errors estimates, etc.) to control as much as
possible the execution. However, this tracking is typically done
only after the simulation ends. This paper presents DfAnalyzer, a
solution based on provenance data to extract and relate strategic
simulation data for online queries. We integrate DfAnalyzer to
libMesh and ParaView Catalyst, so that queries on quantities of
interest are enhanced by in situ visualization.},
  file      = {online:https\://hal-lirmm.ccsd.cnrs.fr/lirmm-01400532/document:PDF},
}

@Article{silva2015analyzing,
  author    = {Silva, V{\'\i}tor and Oliveira, Daniel and Valduriez, Patrick and Mattoso, Marta},
  title     = {Analyzing related raw data files through dataflows},
  journal   = {Concurrency and Computation: Practice and Experience},
  year      = {2015},
  doi       = {10.1002/cpe.3616},
  url       = {http://onlinelibrary.wiley.com/doi/10.1002/cpe.3616/abstract},
  abstract  = {Computer simulations may ingest and generate high numbers of raw data files. Most of these files follow a de facto standard format established by the application domain, for example, Flexible Image Transport System for astronomy. Although these formats are supported by a variety of programming languages, libraries, and programs, analyzing thousands or millions of files requires developing specific programs. Database management systems (DBMS) are not suited for this, because they require loading the raw data and structuring it, which becomes heavy at large scale. Systems like NoDB, RAW, and FastBit have been proposed to index and query raw data files without the overhead of using a database management system. However, these solutions are focused on analyzing one single large file instead of several related files. In this case, when related files are produced and required for analysis, the relationship among elements within file contents must be managed manually, with specific programs to access raw data. Thus, this data management may be time-consuming and error-prone. When computer simulations are managed by a scientific workflow management system (SWfMS), they can take advantage of provenance data to relate and analyze raw data files produced during workflow execution. However, SWfMS registers provenance at a coarse grain, with limited analysis on elements from raw data files. When the SWfMS is dataflow-aware, it can register provenance data and the relationships among elements of raw data files altogether in a database, which is useful to access the contents of a large number of files. In this paper, we propose a dataflow approach for analyzing element data from several related raw data files. Our approach is complementary to the existing single raw data file analysis approaches. We use the Montage workflow from astronomy and a workflow from Oil and Gas domain as data-intensive case studies. Our experimental results for the Montage workflow explore different types of raw data flows like showing all linear transformations involved in projection simulation programs, considering specific mosaic elements from input repositories. The cost for raw data extraction is approximately 3.7% of the total application execution time. Copyright © 2015 John Wiley & Sons, Ltd.},
  publisher = {Wiley Online Library},
}

@InProceedings{dey2014up,
  author    = {Dey, Saumen C and Belhajjame, Khalid and Koop, David and Song, Tianhong and Missier, Paolo and Lud{\"a}scher, Bertram},
  title     = {UP \& DOWN: Improving Provenance Precision by Combining Workflow-and Trace-Level Information.},
  booktitle = {TAPP},
  year      = {2014},
  url       = {https://www.usenix.org/system/files/conference/tapp2014/tapp14_paper_dey.pdf},
  abstract  = {Workflow-level provenance declarations can improve
the precision of coarse provenance traces by reducing the
number of “false” dependencies (not every output of a
step depends on every input). Conversely, fine-grained
execution provenance can be used to improve the precision
of input-output dependencies of workflow actors.
We present a new logic-based approach for improving
provenance precision by combining downward and upward
inference, i.e., from workflows to traces and vice
versa},
  file      = {online:https\://www.usenix.org/system/files/conference/tapp2014/tapp14_paper_dey.pdf:PDF},
}

@Article{silva2017raw,
  author    = {Silva, V{\'\i}tor and Leite, Jos{\'e} and Camata, Jos{\'e} J and de Oliveira, Daniel and Coutinho, Alvaro LGA and Valduriez, Patrick and Mattoso, Marta},
  title     = {Raw data queries during data-intensive parallel workflow execution},
  journal   = {Future Generation Computer Systems},
  year      = {2017},
  doi       = {10.1016/j.future.2017.01.016},
  url       = {http://www.sciencedirect.com/science/article/pii/S0167739X17300237},
  abstract  = {Computer simulations consume and produce huge amounts of raw data files presented in different formats, e.g., HDF5 in computational fluid dynamics simulations. Users often need to analyze domain-specific data based on related data elements from multiple files during the execution of computer simulations. In a raw data analysis, one should identify regions of interest in the data space and retrieve the content of specific related raw data files. Existing solutions, such as FastBit and RAW, are limited to a single raw data file analysis and can only be used after the execution of computer simulations. Scientific Workflow Management Systems (SWMS) can manage the dataflow of computer simulations and register related raw data files at a provenance database. This paper aims to combine the advantages of a dataflow-aware SWMS and the raw data file analysis techniques to allow for queries on raw data file elements that are related, but reside in separate files. We propose a component-based architecture, named as ARMFUL (Analysis of Raw data from Multiple Files) with raw data extraction and indexing techniques, which allows for a direct access to specific elements or regions of raw data space. ARMFUL innovates by using a SWMS provenance database to add a dataflow access path to raw data files. ARMFUL facilitates the invocation of ad-hoc programs and third party tools (e.g., FastBit tool) for raw data analyses. In our experiments, a real parallel computational fluid dynamics is executed, exploring different alternatives of raw data extraction, indexing and analysis.},
  publisher = {Elsevier},
}

@Article{bux2013parallelization,
  author      = {Bux, Marc and Leser, Ulf},
  title       = {Parallelization in Scientific Workflow Management Systems},
  journal     = {arXiv preprint arXiv:1303.7195},
  year        = {2013},
  date        = {2013-03-28},
  eprint      = {1303.7195v1},
  eprintclass = {cs.DC},
  eprinttype  = {arXiv},
  url         = {https://arxiv.org/abs/1303.7195},
  abstract    = {Over the last two decades, scientific workflow management systems (SWfMS) have emerged as a means to facilitate the design, execution, and monitoring of reusable scientific data processing pipelines. At the same time, the amounts of data generated in various areas of science outpaced enhancements in computational power and storage capabilities. This is especially true for the life sciences, where new technologies increased the sequencing throughput from kilobytes to terabytes per day. This trend requires current SWfMS to adapt: Native support for parallel workflow execution must be provided to increase performance; dynamically scalable "pay-per-use" compute infrastructures have to be integrated to diminish hardware costs; adaptive scheduling of workflows in distributed compute environments is required to optimize resource utilization. In this survey we give an overview of parallelization techniques for SWfMS, both in theory and in their realization in concrete systems. We find that current systems leave considerable room for improvement and we propose key advancements to the landscape of SWfMS.},
  file        = {online:http\://arxiv.org/pdf/1303.7195v1:PDF},
  keywords    = {cs.DC, 68N19, C.1.4; D.1.3; D.3.2; J.3},
}

@Article{greisen2002representations,
  author    = {Greisen, Eric W and Calabretta, Mark R},
  title     = {Representations of world coordinates in FITS},
  journal   = {Astronomy \& Astrophysics},
  year      = {2002},
  volume    = {395},
  number    = {3},
  pages     = {1061--1075},
  doi       = {10.1051/0004-6361:20021326},
  url       = {https://www.aanda.org/articles/aa/ps/2002/45/aah3859.ps.gz},
  urldate   = {2017-06-16},
  file      = {:https\://www.aanda.org/articles/aa/pdf/2002/45/aah3859.pdf:PDF},
  publisher = {EDP Sciences},
}

@Article{rew1990netcdf,
  author    = {Rew, Russ and Davis, Glenn},
  title     = {NetCDF: an interface for scientific data access},
  journal   = {IEEE computer graphics and applications},
  year      = {1990},
  volume    = {10},
  number    = {4},
  pages     = {76--82},
  doi       = {10.1109/38.56302},
  url       = {http://ieeexplore.ieee.org/abstract/document/56302/},
  urldate   = {2017-06-16},
  abstract  = {The network common data form (NetCDF), a data abstraction for storing and retrieving multidimensional data, is described. NetCDF is distributed as a software library that provides a concrete implementation of that abstraction. The implementation provides a machine-independent format for representing scientific data. Together, the abstraction, library, and data format support the creation, access, and sharing of scientific information. NetCDF is useful for supporting objects that contain dissimilar kinds of data in a heterogeneous network environment and for writing application software that does not depend on application-specific formats. Independence from particular machine representations is achieved by using a nonproprietary standard for external data representation. The discussion covers NetCDF data abstraction and interface; dimensions, variables, and attributes; direct access and hyperslab access, the NetCDF library; the data format; ncdump and ncgen utilities; experience, usability, and performance; limitations of NetCDF; and future plans.},
  publisher = {IEEE},
}

@Online{hdfgroup2014hdf5,
  author  = {The HDF Group},
  title   = {HDF5},
  year    = {2014},
  url     = {https://support.hdfgroup.org/HDF5/},
  urldate = {2017-06-16},
}

@Article{ludascher2006scientific,
  author    = {Ludäscher, Bertram and Altintas, Ilkay and Berkley, Chad and Higgins, Dan and Jaeger, Efrat and Jones, Matthew and Lee, Edward A. and Tao, Jing and Zhao, Yang},
  title     = {Scientific workflow management and the Kepler system},
  journal   = {Concurrency and Computation: Practice and Experience},
  year      = {2006},
  volume    = {18},
  number    = {10},
  pages     = {1039--1065},
  issn      = {1532-0634},
  doi       = {10.1002/cpe.994},
  url       = {http://dx.doi.org/10.1002/cpe.994},
  abstract  = {Many scientific disciplines are now data and information driven, and new scientific knowledge is often gained by scientists putting together data analysis and knowledge discovery ‘pipelines’. A related trend is that more and more scientific communities realize the benefits of sharing their data and computational services, and are thus contributing to a distributed data and computational community infrastructure (a.k.a. ‘the Grid’). However, this infrastructure is only a means to an end and ideally scientists should not be too concerned with its existence. The goal is for scientists to focus on development and use of what we call scientific workflows. These are networks of analytical steps that may involve, e.g., database access and querying steps, data analysis and mining steps, and many other steps including computationally intensive jobs on high-performance cluster computers. In this paper we describe characteristics of and requirements for scientific workflows as identified in a number of our application projects. We then elaborate on Kepler, a particular scientific workflow system, currently under development across a number of scientific data management projects. We describe some key features of Kepler and its underlying Ptolemy II system, planned extensions, and areas of future research. Kepler is a community-driven, open source project, and we always welcome related projects and new contributors to join. Copyright © 2005 John Wiley & Sons, Ltd.},
  file      = {online:https\://www.cct.lsu.edu/~sidhanti/classes/csc7700/papers/Ledashner05.pdf:PDF},
  keywords  = {scientific workflows, Grid workflows, scientific data management, problem-solving environments, dataflow networks},
  publisher = {John Wiley \& Sons, Ltd.},
}

@Article{deelman2005pegasus,
  author    = {Deelman, Ewa and Singh, Gurmeet and Su, Mei-Hui and Blythe, James and Gil, Yolanda and Kesselman, Carl and Mehta, Gaurang and Vahi, Karan and Berriman, G Bruce and Good, John and others},
  title     = {Pegasus: A framework for mapping complex scientific workflows onto distributed systems},
  journal   = {Scientific Programming},
  year      = {2005},
  volume    = {13},
  number    = {3},
  pages     = {219--237},
  doi       = {10.1155/2005/128026},
  url       = {https://www.hindawi.com/journals/sp/2005/128026/abs/},
  abstract  = {This paper describes the Pegasus framework that can be used to map complex scientific workflows onto distributed resources. Pegasus enables users to represent the workflows at an abstract level without needing to worry about the particulars of the target execution systems. The paper describes general issues in mapping applications and the functionality of Pegasus. We present the results of improving application performance through workflow restructuring which clusters multiple tasks in a workflow into single entities. A real-life astronomy application is used as the basis for the study.},
  file      = {online:http\://downloads.hindawi.com/journals/sp/2005/128026.pdf:PDF},
  publisher = {Hindawi Publishing Corporation},
}

@Article{hull2006taverna,
  author    = {Hull, Duncan and Wolstencroft, Katy and Stevens, Robert and Goble, Carole and Pocock, Mathew R and Li, Peter and Oinn, Tom},
  title     = {Taverna: a tool for building and running workflows of services},
  journal   = {Nucleic acids research},
  year      = {2006},
  volume    = {34},
  number    = {suppl 2},
  pages     = {W729--W732},
  url       = {https://academic.oup.com/nar/article/34/suppl_2/W729/2505722/Taverna-a-tool-for-building-and-running-workflows},
  abstract  = {Taverna is an application that eases the use and integration
of the growing number of molecular biology
tools and databases available on the web, especially
web services. It allows bioinformaticians to construct
workflows or pipelines of services to perform a range
of different analyses, such as sequence analysis
and genome annotation. These high-level workflows
can integrate many different resources into a single
analysis. Taverna is available freely under the terms of
the GNU Lesser General Public License (LGPL) from
http://taverna.sourceforge.net/.},
  file      = {online:https\://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/nar/34/suppl_2/10.1093_nar_gkl320/2/gkl320.pdf?Expires=1497922527&Signature=HMCSEttcKmSAlWpBM~Uri8a1Cul~JS1QnP5SFdHMO00XnhEJEvD-PdYedR-XmnSSyRkBDkbRAyQ3ejd8cM70zOCcIzlOxDjrEZqqh8aq4kWPiusSJNJj0fOBNiKlPCFc6F7vTuFBvnuhVUABblNRbiC5noWHZSEynPv1IrulfDo2VN1kamDn0782LZF6cQ~Pr-SC9Apr3J1zYiatr~VIzd0eLiLpx-FIGy5GfR8oqv5-6I7fSYvBBmKlIPmGT4EthCF0FnpuTqP6LQCjyKN1g5bUXiBhU026jZU6IsBXXmFV-kRnGSGccPlloCcO64GySINTPX660~tIv-bsy2U2mQ__&Key-Pair-Id=APKAIUCZBIA4LVPAVW3Q:PDF},
  publisher = {Oxford Univ Press},
}

@InProceedings{wu2009fastbit,
  author       = {Wu, Kesheng and Ahern, Sean and Bethel, E Wes and Chen, Jacqueline and Childs, Hank and Cormier-Michel, Estelle and Geddes, Cameron and Gu, Junmin and Hagen, Hans and Hamann, Bernd and others},
  title        = {FastBit: interactively searching massive data},
  booktitle    = {Journal of Physics: Conference Series},
  year         = {2009},
  volume       = {180},
  number       = {1},
  organization = {IOP Publishing},
  pages        = {012053},
  url          = {http://iopscience.iop.org/article/10.1088/1742-6596/180/1/012053/meta},
  abstract     = {As scientific instruments and computer simulations produce more and more data, the task
of locating the essential information to gain insight becomes increasingly difficult. FastBit is
an efficient software tool to address this challenge. In this article, we present a summary of
the key underlying technologies, namely bitmap compression, encoding, and binning. Together
these techniques enable FastBit to answer structured (SQL) queries orders of magnitude faster
than popular database systems. To illustrate how FastBit is used in applications, we present
three examples involving a high-energy physics experiment, a combustion simulation, and an
accelerator simulation. In each case, FastBit significantly reduces the response time and enables
interactive exploration on terabytes of data.},
  file         = {online:https\://escholarship.org/uc/item/8gc5w3v2.pdf:PDF},
}

@Article{ogasawara2013chiron,
  author    = {Ogasawara, Eduardo and Dias, Jonas and Silva, Vitor and Chirigati, Fernando and Oliveira, Daniel and Porto, Fabio and Valduriez, Patrick and Mattoso, Marta},
  title     = {Chiron: a parallel engine for algebraic scientific workflows},
  journal   = {Concurrency and Computation: Practice and Experience},
  year      = {2013},
  volume    = {25},
  number    = {16},
  pages     = {2327--2341},
  url       = {http://onlinelibrary.wiley.com/doi/10.1002/cpe.3032/full},
  abstract  = {Large-scale scientific experiments based on computer simulations are typically modeled as scientific workflows, which eases the chaining of different programs. These scientific workflows are defined, executed, and monitored by scientific workflow management systems (SWfMS). As these experiments manage large amounts of data, it becomes critical to execute them in high-performance computing environments, such as clusters, grids, and clouds. However, few SWfMS provide parallel support. The ones that do so are usually labor-intensive for workflow developers and have limited primitives to optimize workflow execution. To address these issues, we developed workflow algebra to specify and enable the optimization of parallel execution of scientific workflows. In this paper, we show how the workflow algebra is efficiently implemented in Chiron, an algebraic based parallel scientific workflow engine. Chiron has a unique native distributed provenance mechanism that enables runtime queries in a relational database. We developed two studies to evaluate the performance of our algebraic approach implemented in Chiron; the first study compares Chiron with different approaches, whereas the second one evaluates the scalability of Chiron. By analyzing the results, we conclude that Chiron is efficient in executing scientific workflows, with the benefits of declarative specification and runtime provenance support. Copyright © 2013 John Wiley & Sons, Ltd.},
  file      = {online:https\://www.researchgate.net/profile/Eduardo_Ogasawara/publication/261181465_Chiron_A_Parallel_Engine_for_Algebraic_Scientific_Workflows/links/5613b0ce08aea962a77fda04.pdf:PDF},
  publisher = {Wiley Online Library},
}

@Article{freire2008provenance,
  author    = {Freire, Juliana and Koop, David and Santos, Emanuele and Silva, Cl{\'a}udio T},
  title     = {Provenance for computational tasks: A survey},
  journal   = {Computing in Science \& Engineering},
  year      = {2008},
  volume    = {10},
  number    = {3},
  doi       = {10.1109/mcse.2008.79},
  url       = {http://ieeexplore.ieee.org/abstract/document/4488060/},
  abstract  = {The problem of systematically capturing and managing provenance for computational tasks has recently received significant attention because of its relevance to a wide range of domains and applications. The authors give an overview of important concepts related to provenance management, so that potential users can make informed decisions when selecting or designing a provenance solution.},
  file      = {online:http\://www.sci.utah.edu/publications/freire08/cise2008a.pdf:PDF},
  publisher = {IEEE},
}

@Online{auletedigitalonline,
  author  = {Lexikon},
  editor  = {Lexikon Editora Digital},
  title   = {Dicionário Aulete Digital},
  year    = {2017},
  url     = {http://www.aulete.com.br/},
  urldate = {2017-06-25},
}

@Unpublished{silva2015propostadoutorado,
  author  = {Vítor Silva},
  title   = {Análise de dados científicos sobre múltiplos arquivos ao longo da geração dos dados},
  year    = {2015},
  date    = {2015-12},
  comment = {https://carinercrs.blogspot.com.br/2011/04/tipos-de-entradas-dos-arquivos-bibtex.html},
}

@Article{dias2015data,
  author    = {Dias, Jonas and Guerra, Gabriel and Rochinha, Fernando and Coutinho, Alvaro LGA and Valduriez, Patrick and Mattoso, Marta},
  title     = {Data-centric iteration in dynamic workflows},
  journal   = {Future Generation Computer Systems},
  year      = {2015},
  volume    = {46},
  pages     = {114--126},
  doi       = {10.1016/j.future.2014.10.021},
  url       = {http://www.sciencedirect.com/science/article/pii/S0167739X14002155},
  abstract  = {Dynamic workflows are scientific workflows to support computational science simulations, typically using dynamic processes based on runtime scientific data analyses. They require the ability of adapting the workflow, at runtime, based on user input and dynamic steering. Supporting data-centric iteration is an important step towards dynamic workflows because user interaction with workflows is iterative. However, current support for iteration in scientific workflows is static and does not allow for changing data at runtime. In this paper, we propose a solution based on algebraic operators and a dynamic execution model to enable workflow adaptation based on user input and dynamic steering. We introduce the concept of iteration lineage that makes provenance data management consistent with dynamic iterative workflow changes. Lineage enables scientists to interact with workflow data and configuration at runtime through an API that triggers steering. We evaluate our approach using a novel and real large-scale workflow for uncertainty quantification on a 640-core cluster. The results show impressive execution time savings from 2.5 to 24 days, compared to non-iterative workflow execution. We verify that the maximum overhead introduced by our iterative model is less than 5% of execution time. Also, our proposed steering algorithms are very efficient and run in less than 1 millisecond, in the worst-case scenario.},
  file      = {online:https\://hal-lirmm.ccsd.cnrs.fr/lirmm-01073638/document:PDF},
  publisher = {Elsevier},
}

@Article{ogasawara2011algebraic,
  author   = {Ogasawara, Eduardo and Dias, Jonas and Oliveira, Daniel and Porto, F{\'a}bio and Valduriez, Patrick and Mattoso, Marta},
  title    = {An algebraic approach for data-centric scientific workflows},
  journal  = {Proc. of VLDB Endowment},
  year     = {2011},
  volume   = {4},
  number   = {12},
  pages    = {1328--1339},
  url      = {http://vldb.org/pvldb/vol4/p1328-ogasawara.pdf},
  abstract = {Scientific workflows have emerged as a basic abstraction for
structuring and executing scientific experiments in computational
environments. In many situations, these workflows are
computationally and data intensive, thus requiring execution in
large-scale parallel computers. However, parallelization of
scientific workflows remains low-level, ad-hoc and laborintensive,
which makes it hard to exploit optimization
opportunities. To address this problem, we propose an algebraic
approach (inspired by relational algebra) and a parallel execution
model that enable automatic optimization of scientific workflows.
We conducted a thorough validation of our approach using both a
real oil exploitation application and synthetic data scenarios. The
experiments were run in Chiron, a data-centric scientific
workflow engine implemented to support our algebraic approach.
Our experiments demonstrate performance improvements of up to
226% compared to an ad-hoc workflow implementation.},
  file     = {online:http\://vldb.org/pvldb/vol4/p1328-ogasawara.pdf:PDF},
}

@InProceedings{zhao2007swift,
  author       = {Zhao, Yong and Hategan, Mihael and Clifford, Ben and Foster, Ian and Von Laszewski, Gregor and Nefedova, Veronika and Raicu, Ioan and Stef-Praun, Tiberiu and Wilde, Michael},
  title        = {Swift: Fast, reliable, loosely coupled parallel computation},
  booktitle    = {Services, 2007 IEEE Congress on},
  year         = {2007},
  organization = {IEEE},
  pages        = {199--206},
  doi          = {10.1109/services.2007.63},
  url          = {http://ieeexplore.ieee.org/abstract/document/4278797/},
  abstract     = {We present Swift, a system that combines a novel scripting language called SwiftScript with a powerful runtime system based on CoG Karajan, Falkon, and Globus to allow for the concise specification, and reliable and efficient execution, of large loosely coupled computations. Swift adopts and adapts ideas first explored in the GriPhyN virtual data system, improving on that system in many regards. We describe the SwiftScript language and its use of XDTM to describe the logical structure of complex file system structures. We also present the Swift runtime system and its use of CoG Karajan, Falkon, and Globus services to dispatch and manage the execution of many tasks in parallel and grid environments. We describe application experiences and performance experiments that quantify the cost of Swift operations.},
  file         = {online:http\://swift-lang.org/papers/pdfs/Swift-SWF07.pdf:PDF},
}

@InProceedings{ikeda2013logical,
  author       = {Ikeda, Robert and Sarma, Akash Das and Widom, Jennifer},
  title        = {Logical provenance in data-oriented workflows?},
  booktitle    = {Data Engineering (ICDE), 2013 IEEE 29th International Conference on},
  year         = {2013},
  organization = {IEEE},
  pages        = {877--888},
  doi          = {10.1109/icde.2013.6544882},
  url          = {http://ieeexplore.ieee.org/abstract/document/6544882/},
  abstract     = {We consider the problem of defining, generating,
and tracing provenance in data-oriented workflows, in which
input data sets are processed by a graph of transformations to
produce output results. We first give a new general definition of
provenance for general transformations, introducing the notions
of correctness, precision, and minimality. We then determine
when properties such as correctness and minimality carry over
from the individual transformations’ provenance to the workflow
provenance. We describe a simple logical-provenance specification
language consisting of attribute mappings and filters. We
provide an algorithm for provenance tracing in workflows where
logical provenance for each transformation is specified using
our language. We consider logical provenance in the relational
setting, observing that for a class of Select-Project-Join (SPJ)
transformations, logical provenance specifications encode minimal
provenance. We have built a prototype system supporting
the features and algorithms presented in the paper, and we report
a few preliminary experimental results.},
  file         = {online:http\://ilpubs.stanford.edu\:8090/1042/1/logprov_short.pdf:PDF},
}

@Comment{jabref-meta: databaseType:biblatex;}
