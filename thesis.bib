% Encoding: UTF-8

@Article{mcphillips2015yesworkflow,
  author   = {McPhillips, Timothy and Song, Tianhong and Kolisnik, Tyler and Aulenbach, Steve and Belhajjame, Khalid and Bocinsky, Kyle and Cao, Yang and Chirigati, Fernando and Dey, Saumen and Freire, Juliana and others},
  title    = {YesWorkflow: a user-oriented, language-independent tool for recovering workflow information from scripts},
  journal  = {arXiv preprint arXiv:1502.02403},
  year     = {2015},
  doi      = {10.2218/ijdc.v10i1.370},
  url      = {https://arxiv.org/abs/1502.02403},
  abstract = {Scientific workflow management systems offer features for composing complex computational pipelines from modular building blocks, for executing the resulting automated workflows, and for recording the provenance of data products resulting from workflow runs. Despite the advantages such features provide, many automated workflows continue to be implemented and executed outside of scientific workflow systems due to the convenience and familiarity of scripting languages (such as Perl, Python, R, and MATLAB), and to the high productivity many scientists experience when using these languages. YesWorkflow is a set of software tools that aim to provide such users of scripting languages with many of the benefits of scientific workflow systems. YesWorkflow requires neither the use of a workflow engine nor the overhead of adapting code to run effectively in such a system. Instead, YesWorkflow enables scientists to annotate existing scripts with special comments that reveal the computational modules and dataflows otherwise implicit in these scripts. YesWorkflow tools extract and analyze these comments, represent the scripts in terms of entities based on the typical scientific workflow model, and provide graphical renderings of this workflow-like view of the scripts. Future versions of YesWorkflow also will allow the prospective provenance of the data products of these scripts to be queried in ways similar to those available to users of scientific workflow systems.},
  file     = {online:https\://arxiv.org/pdf/1502.02403.pdf:PDF},
}

@InProceedings{murta2014noworkflow,
  author       = {Murta, Leonardo and Braganholo, Vanessa and Chirigati, Fernando and Koop, David and Freire, Juliana},
  title        = {noWorkflow: capturing and analyzing provenance of scripts},
  booktitle    = {International Provenance and Annotation Workshop},
  year         = {2014},
  organization = {Springer},
  pages        = {71--83},
  doi          = {10.1007/978-3-319-16462-5_6},
  url          = {https://link.springer.com/chapter/10.1007/978-3-319-16462-5_6},
  abstract     = {We propose noWorkflow, a tool that transparently captures
provenance of scripts and enables reproducibility. Unlike existing approaches,
noWorkflow is non-intrusive and does not require users to
change the way they work – users need not wrap their experiments in
scientific workflow systems, install version control systems, or instrument
their scripts. The tool leverages Software Engineering techniques, such as
abstract syntax tree analysis, reflection, and profiling, to collect different
types of provenance, including detailed information about the underlying
libraries. We describe how noWorkflow captures multiple kinds of
provenance and the different classes of analyses it supports: graph-based
visualization; differencing over provenance trails; and inference queries.},
  file         = {online:https\://vgc.poly.edu/~fchirigati/papers/murta-ipaw2014.pdf:PDF},
}

@InBook{Pimentel2016,
  author    = {Pimentel, Jo{\~a}o Felipe and Dey, Saumen and McPhillips, Timothy and Belhajjame, Khalid and Koop, David and Murta, Leonardo and Braganholo, Vanessa and Lud{\"a}scher, Bertram},
  title     = {Yin {\&} Yang: Demonstrating Complementary Provenance from noWorkflow {\&} YesWorkflow},
  booktitle = {Provenance and Annotation of Data and Processes: 6th International Provenance and Annotation Workshop, IPAW 2016, McLean, VA, USA, June 7-8, 2016, Proceedings},
  year      = {2016},
  publisher = {Springer International Publishing},
  isbn      = {978-3-319-40593-3},
  pages     = {161--165},
  doi       = {10.1007/978-3-319-40593-3_13},
  url       = {http://dx.doi.org/10.1007/978-3-319-40593-3_13},
  abstract  = {The noWorkflow and YesWorkflow toolkits both enable researchers to
capture, store, query, and visualize the provenance of results produced by scripts
that process scientific data. noWorkflow captures prospective provenance representing
the program structure of Python scripts, and retrospective provenance
representing key events observed during script execution. YesWorkflow captures
prospective provenance declared through annotations in the comments of scripts,
and supports key retrospective provenance queries by observing what files were
used or produced by the script. We demonstrate how combining complementary
information gathered by noWorkflow and YesWorkflow enables provenance
queries and data lineage visualizations neither tool can provide on its own.},
  address   = {Cham},
  file      = {online:https\://www.researchgate.net/profile/Vanessa_Braganholo/publication/301650411_Yin_Yang_Demonstrating_Complementary_Provenance_from_noWorkflow_YesWorkflow/links/571fe59d08aeaced788acd1f.pdf:PDF},
}

@InProceedings{silva2016situ,
  author    = {Silva, V{\'\i}tor and Camata, Jos{\'e} and De Oliveira, Daniel and Coutinho, Alvaro and Valduriez, Patrick and Mattoso, Marta},
  title     = {In Situ Data Steering on Sedimentation Simulation with Provenance Data},
  booktitle = {SC: High Performance Computing, Networking, Storage and Analysis},
  year      = {2016},
  url       = {https://hal-lirmm.ccsd.cnrs.fr/lirmm-01400532/},
  abstract  = {Parallel adaptive mesh refinement and coarsening
(AMR) are optimal strategies for tackling large-scale simulations.
libMesh is an open-source finite-element library that supports
parallel AMR and is used in multiphysics applications. In
complex simulation runs, users have to track quantities of
interest (residuals, errors estimates, etc.) to control as much as
possible the execution. However, this tracking is typically done
only after the simulation ends. This paper presents DfAnalyzer, a
solution based on provenance data to extract and relate strategic
simulation data for online queries. We integrate DfAnalyzer to
libMesh and ParaView Catalyst, so that queries on quantities of
interest are enhanced by in situ visualization.},
  file      = {online:https\://hal-lirmm.ccsd.cnrs.fr/lirmm-01400532/document:PDF},
}

@Article{silva2015analyzing,
  author    = {Silva, V{\'\i}tor and Oliveira, Daniel and Valduriez, Patrick and Mattoso, Marta},
  title     = {Analyzing related raw data files through dataflows},
  journal   = {Concurrency and Computation: Practice and Experience},
  year      = {2015},
  doi       = {10.1002/cpe.3616},
  url       = {http://onlinelibrary.wiley.com/doi/10.1002/cpe.3616/abstract},
  abstract  = {Computer simulations may ingest and generate high numbers of raw data files. Most of these files follow a de facto standard format established by the application domain, for example, Flexible Image Transport System for astronomy. Although these formats are supported by a variety of programming languages, libraries, and programs, analyzing thousands or millions of files requires developing specific programs. Database management systems (DBMS) are not suited for this, because they require loading the raw data and structuring it, which becomes heavy at large scale. Systems like NoDB, RAW, and FastBit have been proposed to index and query raw data files without the overhead of using a database management system. However, these solutions are focused on analyzing one single large file instead of several related files. In this case, when related files are produced and required for analysis, the relationship among elements within file contents must be managed manually, with specific programs to access raw data. Thus, this data management may be time-consuming and error-prone. When computer simulations are managed by a scientific workflow management system (SWfMS), they can take advantage of provenance data to relate and analyze raw data files produced during workflow execution. However, SWfMS registers provenance at a coarse grain, with limited analysis on elements from raw data files. When the SWfMS is dataflow-aware, it can register provenance data and the relationships among elements of raw data files altogether in a database, which is useful to access the contents of a large number of files. In this paper, we propose a dataflow approach for analyzing element data from several related raw data files. Our approach is complementary to the existing single raw data file analysis approaches. We use the Montage workflow from astronomy and a workflow from Oil and Gas domain as data-intensive case studies. Our experimental results for the Montage workflow explore different types of raw data flows like showing all linear transformations involved in projection simulation programs, considering specific mosaic elements from input repositories. The cost for raw data extraction is approximately 3.7% of the total application execution time. Copyright © 2015 John Wiley & Sons, Ltd.},
  publisher = {Wiley Online Library},
}

@InProceedings{dey2014up,
  author    = {Dey, Saumen C and Belhajjame, Khalid and Koop, David and Song, Tianhong and Missier, Paolo and Lud{\"a}scher, Bertram},
  title     = {UP \& DOWN: Improving Provenance Precision by Combining Workflow-and Trace-Level Information.},
  booktitle = {TAPP},
  year      = {2014},
  url       = {https://www.usenix.org/system/files/conference/tapp2014/tapp14_paper_dey.pdf},
  abstract  = {Workflow-level provenance declarations can improve
the precision of coarse provenance traces by reducing the
number of “false” dependencies (not every output of a
step depends on every input). Conversely, fine-grained
execution provenance can be used to improve the precision
of input-output dependencies of workflow actors.
We present a new logic-based approach for improving
provenance precision by combining downward and upward
inference, i.e., from workflows to traces and vice
versa},
  file      = {online:https\://www.usenix.org/system/files/conference/tapp2014/tapp14_paper_dey.pdf:PDF},
}

@Article{silva2017raw,
  author    = {Silva, V{\'\i}tor and Leite, Jos{\'e} and Camata, Jos{\'e} J and de Oliveira, Daniel and Coutinho, Alvaro LGA and Valduriez, Patrick and Mattoso, Marta},
  title     = {Raw data queries during data-intensive parallel workflow execution},
  journal   = {Future Generation Computer Systems},
  year      = {2017},
  doi       = {10.1016/j.future.2017.01.016},
  url       = {http://www.sciencedirect.com/science/article/pii/S0167739X17300237},
  abstract  = {Computer simulations consume and produce huge amounts of raw data files presented in different formats, e.g., HDF5 in computational fluid dynamics simulations. Users often need to analyze domain-specific data based on related data elements from multiple files during the execution of computer simulations. In a raw data analysis, one should identify regions of interest in the data space and retrieve the content of specific related raw data files. Existing solutions, such as FastBit and RAW, are limited to a single raw data file analysis and can only be used after the execution of computer simulations. Scientific Workflow Management Systems (SWMS) can manage the dataflow of computer simulations and register related raw data files at a provenance database. This paper aims to combine the advantages of a dataflow-aware SWMS and the raw data file analysis techniques to allow for queries on raw data file elements that are related, but reside in separate files. We propose a component-based architecture, named as ARMFUL (Analysis of Raw data from Multiple Files) with raw data extraction and indexing techniques, which allows for a direct access to specific elements or regions of raw data space. ARMFUL innovates by using a SWMS provenance database to add a dataflow access path to raw data files. ARMFUL facilitates the invocation of ad-hoc programs and third party tools (e.g., FastBit tool) for raw data analyses. In our experiments, a real parallel computational fluid dynamics is executed, exploring different alternatives of raw data extraction, indexing and analysis.},
  publisher = {Elsevier},
}

@Article{bux2013parallelization,
  author      = {Bux, Marc and Leser, Ulf},
  title       = {Parallelization in Scientific Workflow Management Systems},
  journal     = {arXiv preprint arXiv:1303.7195},
  year        = {2013},
  date        = {2013-03-28},
  eprint      = {1303.7195v1},
  eprintclass = {cs.DC},
  eprinttype  = {arXiv},
  url         = {https://arxiv.org/abs/1303.7195},
  abstract    = {Over the last two decades, scientific workflow management systems (SWfMS) have emerged as a means to facilitate the design, execution, and monitoring of reusable scientific data processing pipelines. At the same time, the amounts of data generated in various areas of science outpaced enhancements in computational power and storage capabilities. This is especially true for the life sciences, where new technologies increased the sequencing throughput from kilobytes to terabytes per day. This trend requires current SWfMS to adapt: Native support for parallel workflow execution must be provided to increase performance; dynamically scalable "pay-per-use" compute infrastructures have to be integrated to diminish hardware costs; adaptive scheduling of workflows in distributed compute environments is required to optimize resource utilization. In this survey we give an overview of parallelization techniques for SWfMS, both in theory and in their realization in concrete systems. We find that current systems leave considerable room for improvement and we propose key advancements to the landscape of SWfMS.},
  file        = {online:http\://arxiv.org/pdf/1303.7195v1:PDF},
  keywords    = {cs.DC, 68N19, C.1.4; D.1.3; D.3.2; J.3},
}

@Article{greisen2002representations,
  author    = {Greisen, Eric W and Calabretta, Mark R},
  title     = {Representations of world coordinates in FITS},
  journal   = {Astronomy \& Astrophysics},
  year      = {2002},
  volume    = {395},
  number    = {3},
  pages     = {1061--1075},
  doi       = {10.1051/0004-6361:20021326},
  url       = {https://www.aanda.org/articles/aa/ps/2002/45/aah3859.ps.gz},
  urldate   = {2017-06-16},
  file      = {:https\://www.aanda.org/articles/aa/pdf/2002/45/aah3859.pdf:PDF},
  publisher = {EDP Sciences},
}

@Article{rew1990netcdf,
  author    = {Rew, Russ and Davis, Glenn},
  title     = {NetCDF: an interface for scientific data access},
  journal   = {IEEE computer graphics and applications},
  year      = {1990},
  volume    = {10},
  number    = {4},
  pages     = {76--82},
  doi       = {10.1109/38.56302},
  url       = {http://ieeexplore.ieee.org/abstract/document/56302/},
  urldate   = {2017-06-16},
  abstract  = {The network common data form (NetCDF), a data abstraction for storing and retrieving multidimensional data, is described. NetCDF is distributed as a software library that provides a concrete implementation of that abstraction. The implementation provides a machine-independent format for representing scientific data. Together, the abstraction, library, and data format support the creation, access, and sharing of scientific information. NetCDF is useful for supporting objects that contain dissimilar kinds of data in a heterogeneous network environment and for writing application software that does not depend on application-specific formats. Independence from particular machine representations is achieved by using a nonproprietary standard for external data representation. The discussion covers NetCDF data abstraction and interface; dimensions, variables, and attributes; direct access and hyperslab access, the NetCDF library; the data format; ncdump and ncgen utilities; experience, usability, and performance; limitations of NetCDF; and future plans.},
  publisher = {IEEE},
}

@InProceedings{folk1999hdf5,
  author    = {Folk, Mike and Cheng, Albert and Yates, Kim},
  title     = {HDF5: A file format and I/O library for high performance computing applications},
  booktitle = {Proceedings of Supercomputing},
  year      = {1999},
  volume    = {99},
  pages     = {5--33},
}

@Article{ludascher2006scientific,
  author    = {Ludäscher, Bertram and Altintas, Ilkay and Berkley, Chad and Higgins, Dan and Jaeger, Efrat and Jones, Matthew and Lee, Edward A. and Tao, Jing and Zhao, Yang},
  title     = {Scientific workflow management and the Kepler system},
  journal   = {Concurrency and Computation: Practice and Experience},
  year      = {2006},
  volume    = {18},
  number    = {10},
  pages     = {1039--1065},
  issn      = {1532-0634},
  doi       = {10.1002/cpe.994},
  url       = {http://dx.doi.org/10.1002/cpe.994},
  abstract  = {Many scientific disciplines are now data and information driven, and new scientific knowledge is often gained by scientists putting together data analysis and knowledge discovery ‘pipelines’. A related trend is that more and more scientific communities realize the benefits of sharing their data and computational services, and are thus contributing to a distributed data and computational community infrastructure (a.k.a. ‘the Grid’). However, this infrastructure is only a means to an end and ideally scientists should not be too concerned with its existence. The goal is for scientists to focus on development and use of what we call scientific workflows. These are networks of analytical steps that may involve, e.g., database access and querying steps, data analysis and mining steps, and many other steps including computationally intensive jobs on high-performance cluster computers. In this paper we describe characteristics of and requirements for scientific workflows as identified in a number of our application projects. We then elaborate on Kepler, a particular scientific workflow system, currently under development across a number of scientific data management projects. We describe some key features of Kepler and its underlying Ptolemy II system, planned extensions, and areas of future research. Kepler is a community-driven, open source project, and we always welcome related projects and new contributors to join. Copyright © 2005 John Wiley & Sons, Ltd.},
  file      = {online:https\://www.cct.lsu.edu/~sidhanti/classes/csc7700/papers/Ledashner05.pdf:PDF},
  keywords  = {scientific workflows, Grid workflows, scientific data management, problem-solving environments, dataflow networks},
  publisher = {John Wiley \& Sons, Ltd.},
}

@Article{deelman2005pegasus,
  author    = {Deelman, Ewa and Singh, Gurmeet and Su, Mei-Hui and Blythe, James and Gil, Yolanda and Kesselman, Carl and Mehta, Gaurang and Vahi, Karan and Berriman, G Bruce and Good, John and others},
  title     = {Pegasus: A framework for mapping complex scientific workflows onto distributed systems},
  journal   = {Scientific Programming},
  year      = {2005},
  volume    = {13},
  number    = {3},
  pages     = {219--237},
  doi       = {10.1155/2005/128026},
  url       = {https://www.hindawi.com/journals/sp/2005/128026/abs/},
  abstract  = {This paper describes the Pegasus framework that can be used to map complex scientific workflows onto distributed resources. Pegasus enables users to represent the workflows at an abstract level without needing to worry about the particulars of the target execution systems. The paper describes general issues in mapping applications and the functionality of Pegasus. We present the results of improving application performance through workflow restructuring which clusters multiple tasks in a workflow into single entities. A real-life astronomy application is used as the basis for the study.},
  file      = {online:http\://downloads.hindawi.com/journals/sp/2005/128026.pdf:PDF},
  publisher = {Hindawi Publishing Corporation},
}

@Article{hull2006taverna,
  author    = {Hull, Duncan and Wolstencroft, Katy and Stevens, Robert and Goble, Carole and Pocock, Mathew R and Li, Peter and Oinn, Tom},
  title     = {Taverna: a tool for building and running workflows of services},
  journal   = {Nucleic acids research},
  year      = {2006},
  volume    = {34},
  number    = {suppl 2},
  pages     = {W729--W732},
  url       = {https://academic.oup.com/nar/article/34/suppl_2/W729/2505722/Taverna-a-tool-for-building-and-running-workflows},
  abstract  = {Taverna is an application that eases the use and integration
of the growing number of molecular biology
tools and databases available on the web, especially
web services. It allows bioinformaticians to construct
workflows or pipelines of services to perform a range
of different analyses, such as sequence analysis
and genome annotation. These high-level workflows
can integrate many different resources into a single
analysis. Taverna is available freely under the terms of
the GNU Lesser General Public License (LGPL) from
http://taverna.sourceforge.net/.},
  file      = {online:https\://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/nar/34/suppl_2/10.1093_nar_gkl320/2/gkl320.pdf?Expires=1497922527&Signature=HMCSEttcKmSAlWpBM~Uri8a1Cul~JS1QnP5SFdHMO00XnhEJEvD-PdYedR-XmnSSyRkBDkbRAyQ3ejd8cM70zOCcIzlOxDjrEZqqh8aq4kWPiusSJNJj0fOBNiKlPCFc6F7vTuFBvnuhVUABblNRbiC5noWHZSEynPv1IrulfDo2VN1kamDn0782LZF6cQ~Pr-SC9Apr3J1zYiatr~VIzd0eLiLpx-FIGy5GfR8oqv5-6I7fSYvBBmKlIPmGT4EthCF0FnpuTqP6LQCjyKN1g5bUXiBhU026jZU6IsBXXmFV-kRnGSGccPlloCcO64GySINTPX660~tIv-bsy2U2mQ__&Key-Pair-Id=APKAIUCZBIA4LVPAVW3Q:PDF},
  publisher = {Oxford Univ Press},
}

@InProceedings{wu2009fastbit,
  author       = {Wu, Kesheng and Ahern, Sean and Bethel, E Wes and Chen, Jacqueline and Childs, Hank and Cormier-Michel, Estelle and Geddes, Cameron and Gu, Junmin and Hagen, Hans and Hamann, Bernd and others},
  title        = {FastBit: interactively searching massive data},
  booktitle    = {Journal of Physics: Conference Series},
  year         = {2009},
  volume       = {180},
  organization = {IOP Publishing},
  pages        = {012053},
  url          = {http://iopscience.iop.org/article/10.1088/1742-6596/180/1/012053/meta},
  abstract     = {As scientific instruments and computer simulations produce more and more data, the task
of locating the essential information to gain insight becomes increasingly difficult. FastBit is
an efficient software tool to address this challenge. In this article, we present a summary of
the key underlying technologies, namely bitmap compression, encoding, and binning. Together
these techniques enable FastBit to answer structured (SQL) queries orders of magnitude faster
than popular database systems. To illustrate how FastBit is used in applications, we present
three examples involving a high-energy physics experiment, a combustion simulation, and an
accelerator simulation. In each case, FastBit significantly reduces the response time and enables
interactive exploration on terabytes of data.},
  file         = {online:https\://escholarship.org/uc/item/8gc5w3v2.pdf:PDF},
}

@Article{ogasawara2013chiron,
  author    = {Ogasawara, Eduardo and Dias, Jonas and Silva, Vitor and Chirigati, Fernando and Oliveira, Daniel and Porto, Fabio and Valduriez, Patrick and Mattoso, Marta},
  title     = {Chiron: a parallel engine for algebraic scientific workflows},
  journal   = {Concurrency and Computation: Practice and Experience},
  year      = {2013},
  volume    = {25},
  number    = {16},
  pages     = {2327--2341},
  url       = {http://onlinelibrary.wiley.com/doi/10.1002/cpe.3032/full},
  abstract  = {Large-scale scientific experiments based on computer simulations are typically modeled as scientific workflows, which eases the chaining of different programs. These scientific workflows are defined, executed, and monitored by scientific workflow management systems (SWfMS). As these experiments manage large amounts of data, it becomes critical to execute them in high-performance computing environments, such as clusters, grids, and clouds. However, few SWfMS provide parallel support. The ones that do so are usually labor-intensive for workflow developers and have limited primitives to optimize workflow execution. To address these issues, we developed workflow algebra to specify and enable the optimization of parallel execution of scientific workflows. In this paper, we show how the workflow algebra is efficiently implemented in Chiron, an algebraic based parallel scientific workflow engine. Chiron has a unique native distributed provenance mechanism that enables runtime queries in a relational database. We developed two studies to evaluate the performance of our algebraic approach implemented in Chiron; the first study compares Chiron with different approaches, whereas the second one evaluates the scalability of Chiron. By analyzing the results, we conclude that Chiron is efficient in executing scientific workflows, with the benefits of declarative specification and runtime provenance support. Copyright © 2013 John Wiley & Sons, Ltd.},
  file      = {online:https\://www.researchgate.net/profile/Eduardo_Ogasawara/publication/261181465_Chiron_A_Parallel_Engine_for_Algebraic_Scientific_Workflows/links/5613b0ce08aea962a77fda04.pdf:PDF},
  publisher = {Wiley Online Library},
}

@Article{freire2008provenance,
  author    = {Freire, Juliana and Koop, David and Santos, Emanuele and Silva, Cl{\'a}udio T},
  title     = {Provenance for computational tasks: A survey},
  journal   = {Computing in Science \& Engineering},
  year      = {2008},
  volume    = {10},
  number    = {3},
  doi       = {10.1109/mcse.2008.79},
  url       = {http://ieeexplore.ieee.org/abstract/document/4488060/},
  abstract  = {The problem of systematically capturing and managing provenance for computational tasks has recently received significant attention because of its relevance to a wide range of domains and applications. The authors give an overview of important concepts related to provenance management, so that potential users can make informed decisions when selecting or designing a provenance solution.},
  file      = {online:http\://www.sci.utah.edu/publications/freire08/cise2008a.pdf:PDF},
  publisher = {IEEE},
}

@Article{aulete2014dicionario,
  author  = {AULETE, DICION{\'A}RIO and VALENTE, AL dos S},
  title   = {Dicion{\'a}rio online Caldas Aulete},
  journal = {Aulete Digital},
  year    = {2014},
  volume  = {9},
  url     = {http://www.aulete.com.br/},
}

@Unpublished{silva2015propostadoutorado,
  author = {Vítor Silva},
  title  = {Análise de dados científicos sobre múltiplos arquivos ao longo da geração dos dados},
  year   = {2015},
  date   = {2015-12},
  note   = {Qualificação de Doutorado de Vítor Silva.},
}

@Article{dias2015data,
  author    = {Dias, Jonas and Guerra, Gabriel and Rochinha, Fernando and Coutinho, Alvaro LGA and Valduriez, Patrick and Mattoso, Marta},
  title     = {Data-centric iteration in dynamic workflows},
  journal   = {Future Generation Computer Systems},
  year      = {2015},
  volume    = {46},
  pages     = {114--126},
  doi       = {10.1016/j.future.2014.10.021},
  url       = {http://www.sciencedirect.com/science/article/pii/S0167739X14002155},
  abstract  = {Dynamic workflows are scientific workflows to support computational science simulations, typically using dynamic processes based on runtime scientific data analyses. They require the ability of adapting the workflow, at runtime, based on user input and dynamic steering. Supporting data-centric iteration is an important step towards dynamic workflows because user interaction with workflows is iterative. However, current support for iteration in scientific workflows is static and does not allow for changing data at runtime. In this paper, we propose a solution based on algebraic operators and a dynamic execution model to enable workflow adaptation based on user input and dynamic steering. We introduce the concept of iteration lineage that makes provenance data management consistent with dynamic iterative workflow changes. Lineage enables scientists to interact with workflow data and configuration at runtime through an API that triggers steering. We evaluate our approach using a novel and real large-scale workflow for uncertainty quantification on a 640-core cluster. The results show impressive execution time savings from 2.5 to 24 days, compared to non-iterative workflow execution. We verify that the maximum overhead introduced by our iterative model is less than 5% of execution time. Also, our proposed steering algorithms are very efficient and run in less than 1 millisecond, in the worst-case scenario.},
  file      = {online:https\://hal-lirmm.ccsd.cnrs.fr/lirmm-01073638/document:PDF},
  publisher = {Elsevier},
}

@Article{ogasawara2011algebraic,
  author   = {Ogasawara, Eduardo and Dias, Jonas and Oliveira, Daniel and Porto, F{\'a}bio and Valduriez, Patrick and Mattoso, Marta},
  title    = {An algebraic approach for data-centric scientific workflows},
  journal  = {Proc. of VLDB Endowment},
  year     = {2011},
  volume   = {4},
  number   = {12},
  pages    = {1328--1339},
  url      = {http://vldb.org/pvldb/vol4/p1328-ogasawara.pdf},
  abstract = {Scientific workflows have emerged as a basic abstraction for
structuring and executing scientific experiments in computational
environments. In many situations, these workflows are
computationally and data intensive, thus requiring execution in
large-scale parallel computers. However, parallelization of
scientific workflows remains low-level, ad-hoc and laborintensive,
which makes it hard to exploit optimization
opportunities. To address this problem, we propose an algebraic
approach (inspired by relational algebra) and a parallel execution
model that enable automatic optimization of scientific workflows.
We conducted a thorough validation of our approach using both a
real oil exploitation application and synthetic data scenarios. The
experiments were run in Chiron, a data-centric scientific
workflow engine implemented to support our algebraic approach.
Our experiments demonstrate performance improvements of up to
226% compared to an ad-hoc workflow implementation.},
  file     = {online:http\://vldb.org/pvldb/vol4/p1328-ogasawara.pdf:PDF},
}

@InProceedings{zhao2007swift,
  author       = {Zhao, Yong and Hategan, Mihael and Clifford, Ben and Foster, Ian and Von Laszewski, Gregor and Nefedova, Veronika and Raicu, Ioan and Stef-Praun, Tiberiu and Wilde, Michael},
  title        = {Swift: Fast, reliable, loosely coupled parallel computation},
  booktitle    = {Services, 2007 IEEE Congress on},
  year         = {2007},
  organization = {IEEE},
  pages        = {199--206},
  doi          = {10.1109/services.2007.63},
  url          = {http://ieeexplore.ieee.org/abstract/document/4278797/},
  abstract     = {We present Swift, a system that combines a novel scripting language called SwiftScript with a powerful runtime system based on CoG Karajan, Falkon, and Globus to allow for the concise specification, and reliable and efficient execution, of large loosely coupled computations. Swift adopts and adapts ideas first explored in the GriPhyN virtual data system, improving on that system in many regards. We describe the SwiftScript language and its use of XDTM to describe the logical structure of complex file system structures. We also present the Swift runtime system and its use of CoG Karajan, Falkon, and Globus services to dispatch and manage the execution of many tasks in parallel and grid environments. We describe application experiences and performance experiments that quantify the cost of Swift operations.},
  file         = {online:http\://swift-lang.org/papers/pdfs/Swift-SWF07.pdf:PDF},
}

@InProceedings{ikeda2013logical,
  author       = {Ikeda, Robert and Sarma, Akash Das and Widom, Jennifer},
  title        = {Logical provenance in data-oriented workflows?},
  booktitle    = {Data Engineering (ICDE), 2013 IEEE 29th International Conference on},
  year         = {2013},
  organization = {IEEE},
  pages        = {877--888},
  doi          = {10.1109/icde.2013.6544882},
  url          = {http://ieeexplore.ieee.org/abstract/document/6544882/},
  abstract     = {We consider the problem of defining, generating,
and tracing provenance in data-oriented workflows, in which
input data sets are processed by a graph of transformations to
produce output results. We first give a new general definition of
provenance for general transformations, introducing the notions
of correctness, precision, and minimality. We then determine
when properties such as correctness and minimality carry over
from the individual transformations’ provenance to the workflow
provenance. We describe a simple logical-provenance specification
language consisting of attribute mappings and filters. We
provide an algorithm for provenance tracing in workflows where
logical provenance for each transformation is specified using
our language. We consider logical provenance in the relational
setting, observing that for a class of Select-Project-Join (SPJ)
transformations, logical provenance specifications encode minimal
provenance. We have built a prototype system supporting
the features and algorithms presented in the paper, and we report
a few preliminary experimental results.},
  file         = {online:http\://ilpubs.stanford.edu\:8090/1042/1/logprov_short.pdf:PDF},
}

@InProceedings{hendrix2016tigres,
  author       = {Hendrix, Valerie and Fox, James and Ghoshal, Devarshi and Ramakrishnan, Lavanya},
  title        = {Tigres workflow library: Supporting scientific pipelines on hpc systems},
  booktitle    = {Cluster, Cloud and Grid Computing (CCGrid), 2016 16th IEEE/ACM International Symposium on},
  year         = {2016},
  organization = {IEEE},
  pages        = {146--155},
  doi          = {10.1109/ccgrid.2016.54},
  url          = {http://ieeexplore.ieee.org/abstract/document/7515681/},
  abstract     = {The growth in scientific data volumes has resulted in the need for new tools that enable users to operate on and analyze data on large-scale resources. In the last decade, a number of scientific workflow tools have emerged. These tools often target distributed environments, and often need expert help to compose and execute the workflows. Data-intensive workflows are often ad-hoc, they involve an iterative development process that includes users composing and testing their workflows on desktops, and scaling up to larger systems. In this paper, we present the design and implementation of Tigres, a workflow library that supports the iterative workflow development cycle of data-intensive workflows. Tigres provides an application programming interface to a set of programming templates i.e., sequence, parallel, split, merge, that can be used to compose and execute computational and data pipelines. We discuss the results of our evaluation of scientific and synthetic workflows showing Tigres performs with minimal template overheads (mean of 13 seconds over all experiments). We also discuss various factors (e.g., I/O performance, execution mechansims) that affect the performance of scientific workflows on HPC systems.},
  file         = {online:https\://escholarship.org/uc/item/3z6972w3.pdf:PDF},
}

@InProceedings{alagiannis2012nodb,
  author       = {Alagiannis, Ioannis and Borovica, Renata and Branco, Miguel and Idreos, Stratos and Ailamaki, Anastasia},
  title        = {NoDB: efficient query execution on raw data files},
  booktitle    = {Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data},
  year         = {2012},
  organization = {ACM},
  pages        = {241--252},
  doi          = {10.1145/2213836.2213864},
  url          = {https://dl.acm.org/citation.cfm?id=2213864},
  abstract     = {As data collections become larger and larger, data loading evolves to a major bottleneck. Many applications already avoid using database systems, e.g., scientific data analysis and social networks, due to the complexity and the increased data-to-query time. For such applications data collections keep growing fast, even on a daily basis, and we are already in the era of data deluge where we have much more data than what we can move, store, let alone analyze.

Our contribution in this paper is the design and roadmap of a new paradigm in database systems, called NoDB, which do not require data loading while still maintaining the whole feature set of a modern database system. In particular, we show how to make raw data files a first-class citizen, fully integrated with the query engine. Through our design and lessons learned by implementing the NoDB philosophy over a modern DBMS, we discuss the fundamental limitations as well as the strong opportunities that such a research path brings. We identify performance bottlenecks specific for in situ processing, namely the repeated parsing and tokenizing overhead and the expensive data type conversion costs. To address these problems, we introduce an adaptive indexing mechanism that maintains positional information to provide efficient access to raw data files, together with a flexible caching structure.

Our implementation over PostgreSQL, called PostgresRaw, is able to avoid the loading cost completely, while matching the query performance of plain PostgreSQL and even outperforming it in many cases. We conclude that NoDB systems are feasible to design and implement over modern database architectures, bringing an unprecedented positive effect in usability and performance.},
  file         = {online:https\://infoscience.epfl.ch/record/175803/files/NoDBsigmod2012.pdf:PDF},
}

@Article{karpathiotakis2014adaptive,
  author    = {Karpathiotakis, Manos and Branco, Miguel and Alagiannis, Ioannis and Ailamaki, Anastasia},
  title     = {Adaptive query processing on RAW data},
  journal   = {Proceedings of the VLDB Endowment},
  year      = {2014},
  volume    = {7},
  number    = {12},
  pages     = {1119--1130},
  doi       = {10.14778/2732977.2732986},
  url       = {https://dl.acm.org/citation.cfm?id=2732986},
  abstract  = {Database systems deliver impressive performance for large classes of workloads as the result of decades of research into optimizing database engines. High performance, however, is achieved at the cost of versatility. In particular, database systems only operate efficiently over loaded data, i.e., data converted from its original raw format into the system's internal data format. At the same time, data volume continues to increase exponentially and data varies increasingly, with an escalating number of new formats. The consequence is a growing impedance mismatch between the original structures holding the data in the raw files and the structures used by query engines for efficient processing. In an ideal scenario, the query engine would seamlessly adapt itself to the data and ensure efficient query processing regardless of the input data formats, optimizing itself to each instance of a file and of a query by leveraging information available at query time. Today's systems, however, force data to adapt to the query engine during data loading.

This paper proposes adapting the query engine to the formats of raw data. It presents RAW, a prototype query engine which enables querying heterogeneous data sources transparently. RAW employs Just-In-Time access paths, which efficiently couple heterogeneous raw files to the query engine and reduce the overheads of traditional general-purpose scan operators. There are, however, inherent overheads with accessing raw data directly that cannot be eliminated, such as converting the raw values. Therefore, RAW also uses column shreds, ensuring that we pay these costs only for the subsets of raw data strictly needed by a query. We use RAW in a real-world scenario and achieve a two-order of magnitude speedup against the existing hand-written solution.},
  file      = {online:https\://infoscience.epfl.ch/record/200346/files/raw.pdf:PDF},
  publisher = {VLDB Endowment},
}

@InProceedings{chou2011parallel,
  author       = {Chou, Jerry and Wu, Kesheng and Rubel, Oliver and Howison, Mark and Qiang, Ji and Austin, Brian and Bethel, E Wes and Ryne, Rob D and Shoshani, Arie and others},
  title        = {Parallel index and query for large scale data analysis},
  booktitle    = {High Performance Computing, Networking, Storage and Analysis (SC), 2011 International Conference for},
  year         = {2011},
  organization = {IEEE},
  pages        = {1--11},
  doi          = {10.1145/2063384.2063424},
  url          = {http://ieeexplore.ieee.org/abstract/document/6114446/},
  abstract     = {Modern scientific datasets present numerous data management and analysis challenges. State-of-the-art index and query technologies are critical for facilitating interactive exploration of large datasets, but numerous challenges remain in terms of designing a system for processing general scientific datasets. The system needs to be able to run on distributed multi-core platforms, efficiently utilize underlying I/O infrastructure, and scale to massive datasets. We present FastQuery, a novel software framework that address these challenges. FastQuery utilizes a state-of-the- art index and query technology (FastBit) and is designed to process massive datasets on modern supercomputing plat- forms. We apply FastQuery to processing of a massive 50TB dataset generated by a large scale accelerator modeling code. We demonstrate the scalability of the tool to 11,520 cores. Motivated by the scientific need to search for interesting particles in this dataset, we use our framework to reduce search time from hours to tens of seconds.},
  file         = {online:https\://escholarship.org/uc/item/4m416219.pdf:PDF},
}

@InProceedings{blanas2014parallel,
  author       = {Blanas, Spyros and Wu, Kesheng and Byna, Surendra and Dong, Bin and Shoshani, Arie},
  title        = {Parallel data analysis directly on scientific file formats},
  booktitle    = {Proceedings of the 2014 ACM SIGMOD international conference on Management of data},
  year         = {2014},
  organization = {ACM},
  pages        = {385--396},
  doi          = {10.1145/2588555.2612185},
  url          = {https://dl.acm.org/citation.cfm?id=2612185},
  abstract     = {Scientific experiments and large-scale simulations produce massive amounts of data. Many of these scientific datasets are arrays, and are stored in file formats such as HDF5 and NetCDF. Although scientific data management systems, such as SciDB, are designed to manipulate arrays, there are challenges in integrating these systems into existing analysis workflows. Major barriers include the expensive task of preparing and loading data before querying, and converting the final results to a format that is understood by the existing post-processing and visualization tools. As a consequence, integrating a data management system into an existing scientific data analysis workflow is time-consuming and requires extensive user involvement. In this paper, we present the design of a new scientific data analysis system that efficiently processes queries directly over data stored in the HDF5 file format. This design choice eliminates the tedious and error-prone data loading process, and makes the query results readily available to the next processing steps of the analysis workflow. Our design leverages the increasing main memory capacities found in supercomputers through bitmap indexing and in-memory query execution. In addition, query processing over the HDF5 data format can be effortlessly parallelized to utilize the ample concurrency available in large-scale supercomputers and modern parallel file systems. We evaluate the performance of our system on a large supercomputing system and experiment with both a synthetic dataset and a real cosmology observation dataset. Our system frequently outperforms the relational database system that the cosmology team currently uses, and is more than 10X faster than Hive when processing data in parallel. Overall, by eliminating the data loading step, our query processing system is more effective in supporting in situ scientific analysis workflows.},
  file         = {online:http\://web.cse.ohio-state.edu/~blanas.2/files/sigmod2014_analysisonhdf5.pdf:PDF},
}

@Book{ozsu2011principles,
  author    = {{\"O}zsu, M Tamer and Valduriez, Patrick},
  title     = {Principles of distributed database systems},
  year      = {2011},
  publisher = {Springer Science \& Business Media},
  url       = {https://cs.uwaterloo.ca/~tozsu/ddbook/},
  address   = {University of Waterloo},
}

@Article{boncz2008breaking,
  author    = {Boncz, Peter A and Kersten, Martin L and Manegold, Stefan},
  title     = {Breaking the memory wall in MonetDB},
  journal   = {Communications of the ACM},
  year      = {2008},
  volume    = {51},
  number    = {12},
  pages     = {77--85},
  doi       = {10.1145/1409360.1409380},
  url       = {https://dl.acm.org/citation.cfm?id=1409380},
  abstract  = {In the past decades, advances in speed of commodity CPUs have far outpaced advances in RAM latency. Main-memory access has therefore become a performance bottleneck for many computer applications; a phenomenon that is widely known as the "memory wall." In this paper, we report how research around the MonetDB database system has led to a redesign of database architecture in order to take advantage of modern hardware, and in particular to avoid hitting the memory wall. This encompasses (i) a redesign of the query execution model to better exploit pipelined CPU architectures and CPU instruction caches; (ii) the use of columnar rather than row-wise data storage to better exploit CPU data caches; (iii) the design of new cache-conscious query processing algorithms; and (iv) the design and automatic calibration of memory cost models to choose and tune these cache-conscious algorithms in the query optimizer.},
  publisher = {ACM},
}

@Article{kirk2006libmesh,
  author    = {Kirk, Benjamin S and Peterson, John W and Stogner, Roy H and Carey, Graham F},
  title     = {libMesh: a C++ library for parallel adaptive mesh refinement/coarsening simulations},
  journal   = {Engineering with Computers},
  year      = {2006},
  volume    = {22},
  number    = {3-4},
  pages     = {237--254},
  doi       = {10.1007/s00366-006-0049-3},
  url       = {https://link.springer.com/article/10.1007/s00366-006-0049-3},
  abstract  = {In this paper we describe the libMesh (http://libmesh.sourceforge.net) framework for parallel adaptive finite element applications. libMesh is an open-source software library that has been developed to facilitate serial and parallel simulation of multiscale, multiphysics applications using adaptive mesh refinement and coarsening strategies. The main software development is being carried out in the CFDLab (http://cfdlab.ae.utexas.edu) at the University of Texas, but as with other open-source software projects; contributions are being made elsewhere in the US and abroad. The main goals of this article are: (1) to provide a basic reference source that describes libMesh and the underlying philosophy and software design approach; (2) to give sufficient detail and references on the adaptive mesh refinement and coarsening (AMR/C) scheme for applications analysts and developers; and (3) to describe the parallel implementation and data structures with supporting discussion of domain decomposition, message passing, and details related to dynamic repartitioning for parallel AMR/C. Other aspects related to C++ programming paradigms, reusability for diverse applications, adaptive modeling, physics-independent error indicators, and similar concepts are briefly discussed. Finally, results from some applications using the library are presented and areas of future research are discussed.},
  file      = {online:https\://www.researchgate.net/profile/Roy_Stogner/publication/220677642_libMesh_A_C_Library_for_Parallel_Adaptive_Mesh_RefinementCoarsening_Simulations/links/02e7e5346c12916e8a000000/libMesh-A-C-Library-for-Parallel-Adaptive-Mesh-Refinement-Coarsening-Simulations.pdf:PDF},
  publisher = {Springer},
}

@InProceedings{ayachit2015paraview,
  author       = {Ayachit, Utkarsh and Bauer, Andrew and Geveci, Berk and O'Leary, Patrick and Moreland, Kenneth and Fabian, Nathan and Mauldin, Jeffrey},
  title        = {Paraview catalyst: Enabling in situ data analysis and visualization},
  booktitle    = {Proceedings of the First Workshop on In Situ Infrastructures for Enabling Extreme-Scale Analysis and Visualization},
  year         = {2015},
  organization = {ACM},
  pages        = {25--29},
  doi          = {10.1145/2828612.2828624},
  url          = {https://dl.acm.org/citation.cfm?id=2828624},
  abstract     = {Computer simulations are growing in sophistication and producing results of ever greater fidelity. This trend has been enabled by advances in numerical methods and increasing computing power. Yet these advances come with several costs including massive increases in data size, difficulties examining output data, challenges in configuring simulation runs, and difficulty debugging running codes. Interactive visualization tools, like ParaView, have been used for post-processing of simulation results. However, the increasing data sizes, and limited storage and bandwidth make high fidelity post-processing impractical. In situ analysis is recognized as one of the ways to address these challenges. In situ analysis moves some of the post-processing tasks in line with the simulation code thus short circuiting the need to communicate the data between the simulation and analysis via storage. ParaView Catalyst is a data processing and visualization library that enables in situ analysis and visualization. Built on and designed to interoperate with the standard visualization toolkit VTK and the ParaView application, Catalyst enables simulations to intelligently perform analysis, generate relevant output data, and visualize results concurrent with a running simulation. In this paper, we provide an overview of the Catalyst framework and some of the success stories.},
  file         = {online:http\://www.xvis.org/images/a/a2/ISAV2015.pdf:PDF},
}

@Misc{url:monetdb,
  author    = {MonetDB},
  title     = {MonetDB - An Open-Source DataData System},
  url       = {https://www.monetdb.org/},
  timestamp = {2017.07.24},
}

@Comment{jabref-meta: databaseType:biblatex;}
